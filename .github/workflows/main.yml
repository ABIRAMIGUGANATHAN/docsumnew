name: Document Summarization CI/CD

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

jobs:
  setup-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set Environment Variables
        run: |
          echo "MAX_INPUT_TOKENS=1024" >> $GITHUB_ENV
          echo "MAX_TOTAL_TOKENS=2048" >> $GITHUB_ENV
          echo "no_proxy=${{ secrets.NO_PROXY }},${{ secrets.HOST_IP }}" >> $GITHUB_ENV
          echo "MEGA_SERVICE_HOST_IP=${{ secrets.HOST_IP }}" >> $GITHUB_ENV
          echo "LLM_SERVICE_HOST_IP=${{ secrets.HOST_IP }}" >> $GITHUB_ENV
          echo "ASR_SERVICE_HOST_IP=${{ secrets.HOST_IP }}" >> $GITHUB_ENV
          echo "LLM_MODEL_ID=Intel/neural-chat-7b-v3-3" >> $GITHUB_ENV
          echo "BACKEND_SERVICE_ENDPOINT=http://${{ secrets.HOST_IP }}:8888/v1/docsum" >> $GITHUB_ENV
          echo "LLM_ENDPOINT_PORT=8008" >> $GITHUB_ENV
          echo "DOCSUM_PORT=9000" >> $GITHUB_ENV
          echo "LLM_ENDPOINT=http://${{ secrets.HOST_IP }}:8008" >> $GITHUB_ENV
          echo "DocSum_COMPONENT_NAME=OpeaDocSumTgi" >> $GITHUB_ENV
          echo "HOST_IP=${{ secrets.HOST_IP }}" >> $GITHUB_ENV
          echo "NO_PROXY=${{ secrets.NO_PROXY }}" >> $GITHUB_ENV
          echo "HUGGINGFACEHUB_API_TOKEN=${{ secrets.HUGGINGFACEHUB_API_TOKEN }}" >> $GITHUB_ENV

      - name: Deploy Using Docker
        run: |
          COMPOSE_PATH="docker_compose/intel/cpu/xeon/compose.yaml"
          docker compose -f $COMPOSE_PATH down  # Stop existing containers
          docker compose -f $COMPOSE_PATH up -d

      - name: Wait for Service to Start
        run: sleep 30

  test-document-summarization:
    needs: setup-and-deploy
    runs-on: ubuntu-latest
    steps:
      - name: Test Text Summarization (English)
        run: |
          curl -X POST http://${{ env.HOST_IP }}:8888/v1/docsum \
               -H "Content-Type: application/json" \
               -d '{"type": "text", "messages": "Text Embeddings Inference (TEI) is a toolkit for deploying and serving open source text embeddings and sequence classification models. TEI enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE and E5."}'

      - name: Cleanup
        if: always()
        run: |
          cd docsumnew/docker_compose/intel/cpu/xeon/
          docker compose -f compose.yaml down
