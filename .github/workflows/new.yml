name: Deploy and Test Document Summarization Service

on:
  workflow_dispatch:

jobs:
  deploy-docsum:
    runs-on: ubuntu-latest

    env:
      LLM_MODEL_ID: "Intel/neural-chat-7b-v3-3"
      host_ip: "192.168.2.209"
      no_proxy: "localhost,127.0.0.1,192.168.2.209"
      HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set environment variables
        run: |
          echo "LLM_MODEL_ID=${{ env.LLM_MODEL_ID }}" >> $GITHUB_ENV
          echo "host_ip=${{ env.host_ip }}" >> $GITHUB_ENV
          echo "no_proxy=${{ env.no_proxy }}" >> $GITHUB_ENV
          echo "HUGGINGFACEHUB_API_TOKEN=${{ env.HUGGINGFACEHUB_API_TOKEN }}" >> $GITHUB_ENV

      - name: Deploy Document Summarization on Xeon
        run: |
          COMPOSE_PATH="docsum/docker_compose/intel/cpu/xeon/compose.yaml"
          docker compose up -d

      - name: Wait for service to start
        run: sleep 60

      - name: Test DocSum with cURL
        run: |
          curl -X POST http://${{ env.host_ip }}:8888/v1/docsum \
               -H "Content-Type: application/json" \
               -d '{"type": "text", "messages": "Text Embeddings Inference (TEI) is a toolkit for deploying and serving open source text embeddings and sequence classification models. TEI enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE and E5."}'
